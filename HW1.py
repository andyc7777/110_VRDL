# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6ha7lcCtaSvmFoJc2obDDt3-6cCAU3D

# Requirements

##### To use ensemble methon, install torchensemble.
"""
import os
import cv2
import numpy as np
import pandas as pd
import glob
import shutil
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torchvision import transforms, utils
from torchvision.transforms import transforms
from torch.utils.data import Dataset, ConcatDataset, DataLoader, Subset
import pathlib
from torchvision.datasets import DatasetFolder
from tqdm.auto import tqdm
from torch.utils.data import random_split

import torchvision.models as models

from torchensemble import VotingClassifier
from torchensemble.utils.logging import set_logger
from torchensemble.utils.io import load

!pip install torchensemble

"""# Import Packages

##### Importing needed packages.
"""


"""# Download Dataset and Unzip"""

# Google Drive
# Code ref. from Homework of NTU Prof.Hung-Yi Lee's lecture
!gdown --id '1aerVHZJo5GTRU-06PICcCvl39Y-VR3rz' --output 2021VRDL_HW1_datasets.zip

!apt-get install unzi

!unzip '2021VRDL_HW1_datasets.zip' -d 2021VRDL_HW1_datasets

os.chdir("/content/2021VRDL_HW1_datasets/")

!unzip 'training_images.zip' -d training_images
!unzip 'testing_images.zip' -d testing_images

"""# Data pre-processing"""

# 提取class列跟對應的label
classes = pd.read_csv('classes.txt', sep=" ", header=None)
classes.columns = ['class']

classes_num = []  # 把class的編號跟名稱存成dict，以便查詢
classes_name = []
for index, row in classes.iterrows():
    classes_num.append(str(row['class'])[0:3])
    classes_name.append(str(row['class'])[4:])
class_dict = dict(zip(classes_num, classes_name))

"""# Dataset Processing

##### Read the pictures, do data augmentation, and put them into dataset and dataloader.
##### In the data augmentation part, I choose six transform methods, and concat them with the original data.
##### The part of dataset code ref. from https://www.cnblogs.com/denny402/p/7512516.html

"""


def default_loader(path):
    return Image.open(path).convert('RGB')

transform_set = [
    transforms.RandomHorizontalFlip(p=1),
    transforms.RandomVerticalFlip(p=1),
    transforms.RandomRotation(30, resample=Image.BICUBIC, expand=False, center=(55, 5)),
    transforms.ColorJitter(brightness=(0, 5), contrast=(0, 5), saturation=(0, 5), hue=(-0.1, 0.1)),
    # transforms.CenterCrop(200),
    transforms.RandomCrop(200),
    transforms.RandomResizedCrop((200, 200)),
    # transforms.TenCrop(200, vertical_flip=False),
    # transforms.GaussianBlur(7,3),
    # transforms.RandomAffine(degrees=(-30,30), translate=(0, 0.5), scale=(0.4, 0.5), shear=(0,0), fillcolor=(0,255,255)),
    # transforms.Grayscale(num_output_channels=3),
    # transforms.RandomGrayscale(p=0.9),
    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=2),

]

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])

transform_aug_1 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=1),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])
transform_aug_2 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomVerticalFlip(p=1),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])
transform_aug_3 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomRotation(30, resample=Image.BICUBIC, expand=False, center=(55, 5)),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406),
                         std=(0.229, 0.224, 0.225)),
])
transform_aug_4 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ColorJitter(brightness=(0, 5), contrast=(0, 5), saturation=(0, 5), hue=(-0.1, 0.1)),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])
transform_aug_5 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomCrop(200),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])
transform_aug_6 = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomResizedCrop((200, 200)),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
])


transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])


class training_set(Dataset):
    def __init__(self, txt, transform=None, target_transform=None, loader=default_loader):
        fh = open(txt, 'r')
        imgs = []
        for line in fh:
            line = line.strip('\n')
            line = line.rstrip()
            words = line.split()
            imgs.append((words[0], int(str(words[1])[0:3])))

        self.imgs = imgs
        self.transform = transform
        self.target_transform = target_transform
        self.loader = loader

    def __getitem__(self, index):
        fn, label = self.imgs[index]
        fn = './training_images/' + fn
        img = self.loader(fn)
        if self.transform is not None:
            img = self.transform(img)
        return img, label

    def __len__(self):
        return len(self.imgs)


class testing_set(Dataset):
    def __init__(self, txt, transform=None, target_transform=None, loader=default_loader):
        fh = open(txt, 'r')
        test_images = []
        for line in fh:
            line = line.strip('\n')
            line = line.rstrip()
            words = line.split()
            test_images.append((words))

        self.test_images = test_images
        self.transform = transform
        self.loader = loader

    def __getitem__(self, index):
        fn = self.test_images[index]
        fn = './testing_images/' + str(fn)[2:10]
        img = self.loader(fn)
        if self.transform is not None:
            img = self.transform(img)
        return img

    def __len__(self):
        return len(self.test_images)

train_data = training_set(txt='training_labels.txt', transform=transform_train)
aug_train_data_1 = training_set(txt='training_labels.txt', transform=transform_aug_1)
aug_train_data_2 = training_set(txt='training_labels.txt', transform=transform_aug_2)
aug_train_data_3 = training_set(txt='training_labels.txt', transform=transform_aug_3)
aug_train_data_4 = training_set(txt='training_labels.txt', transform=transform_aug_4)
aug_train_data_5 = training_set(txt='training_labels.txt', transform=transform_aug_5)
aug_train_data_6 = training_set(txt='training_labels.txt', transform=transform_aug_6)
test_data = testing_set(txt='testing_img_order.txt', transform=transform_test)

train_data = ConcatDataset([train_data, aug_train_data_1, aug_train_data_2, aug_train_data_3, aug_train_data_4, aug_train_data_5, aug_train_data_6])

train_dataset, val_dataset = random_split(
    dataset=train_data,
    lengths=[20000, 1000],
    generator=torch.Generator().manual_seed(42)
)

training_data_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)
val_data_loader = DataLoader(val_dataset, batch_size=50, huffle=True)
testing_data_loader = DataLoader(test_data, batch_size=50, shuffle=False)

print(len(training_data_loader))
print(len(val_data_loader))
print(len(testing_data_loader))


# 看一下batch裡面的圖片
'''def show_batch(imgs):
    grid = utils.make_grid(imgs)
    plt.imshow(grid.numpy().transpose((1, 2, 0)))
    plt.title('Batch from dataloader')


for i, (batch_x) in enumerate(testing_data_loader):
    if(i<4):
        print(i, batch_x.size())
        show_batch(batch_x)
        plt.axis('off')
        plt.show()
'''

"""# Training & Validation
##### Using pretrained model "resnext50_32x4d".
##### Code ref. from Homework of NTU Prof.Hung-Yi Lee's lecture and https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter04_advanced/4_1_fine-tuning/

"""

# ref:https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter04_advanced/4_1_fine-tuning/
resnext50 = models.resnext50_32x4d(pretrained=True)
device = "cuda" if torch.cuda.is_available() else "cpu"
# 凍結參數
for param in resnext50.parameters():
    param.requires_grad = False
# print(resnext50)
# 修改ＦＣ層的輸出
num_ftrs = resnext50.fc.in_features
resnext50.fc = nn.Sequential(
              nn.Linear(num_ftrs, 201)
              )


resnext50 = resnext50.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(resnext50.fc.parameters(), lr=0.001)

n_epochs = 5  # 容易overfitting, epoch少一點
valid_loss_min = np.Inf

for epoch in range(n_epochs):
    # ---------- Training ----------
    # Make sure the model is in train mode before training.
    resnext50.train()

    # These are used to record information in training.
    train_loss = []
    train_accs = []

    # Iterate the training set by batches.
    for batch in tqdm(training_data_loader):

        # A batch consists of image data and corresponding labels.
        imgs, labels = batch

        # Forward the data. (Make sure data and model are on the same device.)
        logits = resnext50(imgs.to(device))

        # Calculate the cross-entropy loss.
        # We don't need to apply softmax before computing cross-entropy as it is done automatically.
        loss = criterion(logits, labels.to(device))

        # Gradients stored in the parameters in the previous step should be cleared out first.
        optimizer.zero_grad()

        # Compute the gradients for parameters.
        loss.backward()

        # Clip the gradient norms for stable training.
        grad_norm = nn.utils.clip_grad_norm_(resnext50.parameters(), max_norm=10)

        # Update the parameters with computed gradients.
        optimizer.step()

        # Compute the accuracy for current batch.
        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        # Record the loss and accuracy.
        train_loss.append(loss.item())
        train_accs.append(acc)

    # The average loss and accuracy of the training set is the average of the recorded values.
    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)

    # Print the information.
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # ---------- Validation ----------
    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.
    resnext50.eval()

    # These are used to record information in validation.
    valid_loss = []
    valid_accs = []

    # Iterate the validation set by batches.
    for batch in tqdm(val_data_loader):

        # A batch consists of image data and corresponding labels.
        imgs, labels = batch

        # We don't need gradient in validation.
        # Using torch.no_grad() accelerates the forward process.
        with torch.no_grad():
            logits = resnext50(imgs.to(device))

        # We can still compute the loss (but not the gradient).
        loss = criterion(logits, labels.to(device))

        # Compute the accuracy for current batch.
        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        # Record the loss and accuracy.
        valid_loss.append(loss.item())
        valid_accs.append(acc)

    # The average loss and accuracy for entire validation set is the average of the recorded values.
    valid_loss = sum(valid_loss) / len(valid_loss)
    valid_acc = sum(valid_accs) / len(valid_accs)

    # Print the information.
    print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

    # save model if validation loss has decreased
    if valid_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
            valid_loss_min,
            valid_loss))
        torch.save(resnext50.state_dict(), 'resnext50.pth')
        valid_loss_min = valid_loss

"""# Inference
##### Code ref. from Homework of NTU Prof.Hung-Yi Lee's lecture.
"""

# Set model to eval mode
resnext50.eval()

device = "cuda" if torch.cuda.is_available() else "cpu"

predictions = []

# Iterate the testing set by batches.
for batch in tqdm(testing_data_loader):
    # A batch consists of image data and corresponding labels.
    # But here the variable "labels" is useless since we do not have the ground-truth.
    # If printing out the labels, you will find that it is always 0.
    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,
    # so we have to create fake labels to make it work normally.
    # print(batch)
    imgs = batch

    # We don't need gradient in testing, and we don't even have labels to compute loss.
    # Using torch.no_grad() accelerates the forward process.
    with torch.no_grad():
        logits = resnext50(imgs.to(device))

    # Take the class with greatest logit as prediction and record it.
    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())

"""# Save Prediction
##### Modify the code from the TA.
##### And some code ref. from https://www.itread01.com/question/aXgwZQ==.html
"""

for i in range(len(predictions)):
    if predictions[i] < 100 and predictions[i] >= 10:
        predictions[i] = '0' + str(predictions[i])
    elif predictions[i] < 10:
        predictions[i] = '00' + str(predictions[i])

fh = open('testing_img_order.txt', 'r')
test_images = []
for line in fh:
    line = line.strip('\n')
    line = line.rstrip()
    words = line.split()
    test_images.append((str(words)[2:10]))


# ref:https://www.itread01.com/question/aXgwZQ==.html
def _finditem(obj, key):
    if key in obj.keys():
        print('有')
        return obj[key]
    for k, v in obj.items():
        if isinstance(v, dict):
            item = _finditem(v, key)
            if item is not None:
                return item


def find(dic, input_key):
    if str(input_key) in dic.keys():
        return dic[str(input_key)]

pred_class = []
for i in range(len(predictions)):
    class_name = find(class_dict, predictions[i])
    pred = str(predictions[i]) + '.' + str(class_name)
    pred_class.append(pred)

submission = {'file name': test_images, 'pred': pred_class}
submission = pd.DataFrame(submission)
submission.to_csv('answer.txt', sep=' ', header=None, index=False)

"""# Ensemble Method Attemption
##### Code ref. from https://ensemble-pytorch.readthedocs.io/en/latest/
"""

# Set the Logger
logger = set_logger('classification')

# Define the ensemble
ens_model = VotingClassifier(
    estimator=resnet50,
    n_estimators=10,
    cuda=True,
)

# Set the optimizer
ens_model.set_optimizer('Adam', lr=1e-3, weight_decay=5e-4)

# Set the learning rate scheduler
ens_model.set_scheduler(
    "CosineAnnealingLR",                    # type of learning rate scheduler
    T_max=10,                           # additional arguments on the scheduler
)

# Train and Evaluate
ens_model.fit(
    training_data_loader,
    epochs=5,
    test_loader=val_data_loader,
)

"""# Load saved Ensemble Model"""

ens_load_model = VotingClassifier(
    estimator=resnet50,
    n_estimators=10,
    cuda=True,
)
load(ens_load_model, './')  # reload

"""# Inference of Ensemble Model
##### Code ref. from Homework of NTU Prof.Hung-Yi Lee's lecture.
"""

# Evaluate the ensemble
device = "cuda" if torch.cuda.is_available() else "cpu"

predictions = []

# Iterate the testing set by batches.
for batch in tqdm(testing_data_loader):
    # A batch consists of image data and corresponding labels.
    # But here the variable "labels" is useless since we do not have the ground-truth.
    # If printing out the labels, you will find that it is always 0.
    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,
    # so we have to create fake labels to make it work normally.
    # print(batch)
    imgs = batch
    # We don't need gradient in testing, and we don't even have labels to compute loss.
    # Using torch.no_grad() accelerates the forward process.
    with torch.no_grad():
        pred = ens_load_model.predict(imgs)

    # Take the class with greatest logit as prediction and record it.
    predictions.extend(pred.argmax(dim=-1).cpu().numpy().tolist())

"""# Save Prediction
##### Modify the code from the TA.
##### And some code ref. from https://www.itread01.com/question/aXgwZQ==.html
"""

for i in range(len(predictions)):
    if predictions[i] < 100 and predictions[i] >= 10:
        predictions[i] = '0' + str(predictions[i])
    elif predictions[i] < 10:
        predictions[i] = '00' + str(predictions[i])

fh = open('testing_img_order.txt', 'r')
test_images = []
for line in fh:
    line = line.strip('\n')
    line = line.rstrip()
    words = line.split()
    test_images.append((str(words)[2:10]))


# ref:https://www.itread01.com/question/aXgwZQ==.html
def _finditem(obj, key):
    if key in obj.keys():
        print('有')
        return obj[key]
    for k, v in obj.items():
        if isinstance(v, dict):
            item = _finditem(v, key)
            if item is not None:
                return item


def find(dic, input_key):
    if str(input_key) in dic.keys():
        return dic[str(input_key)]

pred_class = []
for i in range(len(predictions)):
    class_name = find(class_dict, predictions[i])
    pred = str(predictions[i]) + '.' + str(class_name)
    pred_class.append(pred)

submission = {'file name': test_images, 'pred': pred_class}
submission = pd.DataFrame(submission)
submission.to_csv('ens_answer.txt', sep=' ', header=None, index=False)
